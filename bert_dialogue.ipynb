{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_dialogue.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARTq0BPAq6-V",
        "colab_type": "text"
      },
      "source": [
        "[Open with Colab](https://colab.research.google.com/github/1never/UEC_AIX_seminar2020/blob/master/bert_dialogue.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XziVAOjaMNZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ライブラリのインストール\n",
        "!pip install transformers\n",
        "!git clone https://github.com/huggingface/transformers.git\n",
        "!apt install git make curl xz-utils file\n",
        "!apt install mecab libmecab-dev mecab-ipadic mecab-ipadic-utf8\n",
        "!pip install mecab-python3==0.996.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFcgEOOqMrd1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 応答ペアデータのダウンロードと解凍\n",
        "!wget http://130.153.158.5/~inaba/unno_pair.zip\n",
        "!unzip -u unno_pair.zip \n",
        "\n",
        "#　全データをダウンロードする場合\n",
        "# !wget http://130.153.158.5/~inaba/AIX_seminar2020.zip \n",
        "# !unzip AIX_seminar2020.zip -o\n",
        "\n",
        "# 学習済みモデル保存用フォルダの作成\n",
        "!mkdir bert_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9MD9z0kMk3z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "write_lines = []\n",
        "uttrs = []\n",
        "\n",
        "use_file = \"unno_pair.txt\" # SF作家 海野十三のデータ (6 MB)\n",
        "# use_file = \"aozora_pair.txt\" # 青空文庫のデータ (85 MB)\n",
        "# use_file = \"narou_pair.txt\" # 小説家になろうのデータ (640 MB)．容量が大きいので各処理にかなり時間がかかります．\n",
        "\n",
        "with open(use_file) as f:\n",
        "    for l in f:\n",
        "        l = l.strip()\n",
        "        if \"\\t\" in l:\n",
        "            # 実際の応答ペアを正解とし，ラベルは1とする．\n",
        "            write_lines.append(l + \"\\t1\\n\")\n",
        "            # 不正解ペアの作成のため，発話を保存\n",
        "            uttrs.append(l.split(\"\\t\")[0])\n",
        "            uttrs.append(l.split(\"\\t\")[1])\n",
        "  \n",
        "# 正解ペアと同じ数だけ不正解ペアを作成\n",
        "for i in range(len(write_lines)):\n",
        "    # ランダムな応答ペアを不正解とし，ラベルは0とする．\n",
        "    write_lines.append(random.choice(uttrs) + \"\\t\" + random.choice(uttrs) + \"\\t0\\n\")\n",
        "  \n",
        " # 正解ペアと不正解ペアが入ったリストをシャッフルする\n",
        "random.shuffle(write_lines)\n",
        "  \n",
        "index = 0\n",
        "with open(\"bert_data/dev.tsv\", \"w\") as var_f:\n",
        "    # 開発データとしてdev.tsvに200行を書き込む．\n",
        "    for l in write_lines[:200]:\n",
        "        var_f.write(str(index) + \"\\t\" + l)\n",
        "        index += 1\n",
        "index = 0\n",
        "with open(\"bert_data/train.tsv\", \"w\") as var_f:\n",
        "    # 学習データとしてtrain.tsvにのこりを書き込む．\n",
        "    for l in write_lines[200:]:\n",
        "        var_f.write(str(index) + \"\\t\" + l)\n",
        "        index += 1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBDJbEwYMbzr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python transformers/examples/text-classification/run_glue.py --data_dir bert_data/  --overwrite_output_dir \\\n",
        "--model_name_or_path cl-tohoku/bert-base-japanese-whole-word-masking --task_name WNLI --evaluate_during_training --save_steps 1000 --max_steps 1000 \\\n",
        "--output_dir bert_output/ --do_train --do_eval --per_gpu_train_batch_size 16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQSPUqudPr_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Elasticsearchのダウンロードと解凍\n",
        "!wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.0.0-linux-x86_64.tar.gz -q\n",
        "!tar -xzf elasticsearch-7.0.0-linux-x86_64.tar.gz\n",
        "\n",
        "# Elasticsearchの日本語形態素解析用プラグイン analysis-kuromojiのインストール\n",
        "!elasticsearch-7.0.0/bin/elasticsearch-plugin install analysis-kuromoji\n",
        "\n",
        "# Pythonのelasticsearchライブラリのインストール\n",
        "!pip install elasticsearch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjsRl8G3Pt54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Elasticsearchの実行\n",
        "!pkill -f elasticsearch\n",
        "!chown -R daemon:daemon elasticsearch-7.0.0/bin/\n",
        "!chown -R daemon:daemon elasticsearch-7.0.0/\n",
        "\n",
        "import os\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "es_server = Popen(['elasticsearch-7.0.0/bin/elasticsearch'], stdout=PIPE, stderr=STDOUT, preexec_fn=lambda: os.setuid(1))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0XbB4hnCa0_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "237c1d0b-c734-4380-a927-2af97a49bda2"
      },
      "source": [
        "# 接続テスト (上記セルの実行から30秒ほど待つ必要があります)\n",
        "!curl -X GET \"localhost:9200/\"\n",
        "\n",
        "# Pythonライブラリによる接続テスト\n",
        "from elasticsearch import Elasticsearch, helpers\n",
        "es = Elasticsearch()\n",
        "es.ping()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"name\" : \"6287d806ba36\",\n",
            "  \"cluster_name\" : \"elasticsearch\",\n",
            "  \"cluster_uuid\" : \"lEft9kLhQzWleq4XDdEqyg\",\n",
            "  \"version\" : {\n",
            "    \"number\" : \"7.0.0\",\n",
            "    \"build_flavor\" : \"default\",\n",
            "    \"build_type\" : \"tar\",\n",
            "    \"build_hash\" : \"b7e28a7\",\n",
            "    \"build_date\" : \"2019-04-05T22:55:32.697037Z\",\n",
            "    \"build_snapshot\" : false,\n",
            "    \"lucene_version\" : \"8.0.0\",\n",
            "    \"minimum_wire_compatibility_version\" : \"6.7.0\",\n",
            "    \"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\n",
            "  },\n",
            "  \"tagline\" : \"You Know, for Search\"\n",
            "}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxphzV-1QEZ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "32492daf-aaec-4373-c511-d0dd2daa7f7a"
      },
      "source": [
        "# 対話データをElasticsearchにインサート\n",
        "def load():\n",
        "    try:\n",
        "        es.delete_by_query(index='dialogue_pair', body={\"query\": {\"match_all\": {}}})\n",
        "        print(\"既存データを削除\")\n",
        "    except:\n",
        "        print(\"削除対象データなし\")\n",
        "        pass\n",
        "\n",
        "    with open(use_file) as f:\n",
        "        for i, __ in enumerate(f):\n",
        "            print(i, '...', end='\\r')\n",
        "            __ = __.split('\\t')\n",
        "            query = __[0].strip()\n",
        "            response = __[1].strip()\n",
        "            item = {'_index':'dialogue_pair', '_type':'docs', '_source':{ 'query':query, 'response':response }}\n",
        "            yield item\n",
        "\n",
        "print(helpers.bulk(es, load()))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "既存データを削除\n",
            "0 ...\r1 ...\r2 ...\r3 ...\r4 ...\r5 ...\r6 ...\r7 ...\r8 ...\r9 ...\r10 ...\r11 ...\r12 ...\r13 ...\r14 ...\r15 ...\r16 ...\r17 ...\r18 ...\r19 ...\r20 ...\r21 ...\r22 ...\r23 ...\r24 ...\r25 ...\r26 ...\r27 ...\r28 ...\r29 ...\r30 ...\r31 ...\r32 ...\r33 ...\r34 ...\r35 ...\r36 ...\r37 ...\r38 ...\r39 ...\r40 ...\r41 ...\r42 ...\r43 ...\r44 ...\r45 ...\r46 ...\r47 ...\r48 ...\r49 ...\r50 ...\r51 ...\r52 ...\r53 ...\r54 ...\r55 ...\r56 ...\r57 ...\r58 ...\r59 ...\r60 ...\r61 ...\r62 ...\r63 ...\r64 ...\r65 ...\r66 ...\r67 ...\r68 ...\r69 ...\r70 ...\r71 ...\r72 ...\r73 ...\r74 ...\r75 ...\r76 ...\r77 ...\r78 ...\r79 ...\r80 ...\r81 ...\r82 ...\r83 ...\r84 ...\r85 ...\r86 ...\r87 ...\r88 ...\r89 ...\r90 ...\r91 ...\r92 ...\r93 ...\r94 ...\r95 ...\r96 ...\r97 ...\r98 ...\r99 ...\r100 ...\r101 ...\r102 ...\r103 ...\r104 ...\r105 ...\r106 ...\r107 ...\r108 ...\r109 ...\r110 ...\r111 ...\r112 ...\r113 ...\r114 ...\r115 ...\r116 ...\r117 ...\r118 ...\r119 ...\r120 ...\r121 ...\r122 ...\r123 ...\r124 ...\r125 ...\r126 ...\r127 ...\r128 ...\r129 ...\r130 ...\r131 ...\r132 ...\r133 ...\r134 ...\r135 ...\r136 ...\r137 ...\r138 ...\r139 ...\r140 ...\r141 ...\r142 ...\r143 ...\r144 ...\r145 ...\r146 ...\r147 ...\r148 ...\r149 ...\r150 ...\r151 ...\r152 ...\r153 ...\r154 ...\r155 ...\r156 ...\r157 ...\r158 ...\r159 ...\r160 ...\r161 ...\r162 ...\r163 ...\r164 ...\r165 ...\r166 ...\r167 ...\r168 ...\r169 ...\r170 ...\r171 ...\r172 ...\r173 ...\r174 ...\r175 ...\r176 ...\r177 ...\r178 ...\r179 ...\r180 ...\r181 ...\r182 ...\r183 ...\r184 ...\r185 ...\r186 ...\r187 ...\r188 ...\r189 ...\r190 ...\r191 ...\r192 ...\r193 ...\r194 ...\r195 ...\r196 ...\r197 ...\r198 ...\r199 ...\r200 ...\r201 ...\r202 ...\r203 ...\r204 ...\r205 ...\r206 ...\r207 ...\r208 ...\r209 ...\r210 ...\r211 ...\r212 ...\r213 ...\r214 ...\r215 ...\r216 ...\r217 ...\r218 ...\r219 ...\r220 ...\r221 ...\r222 ...\r223 ...\r224 ...\r225 ...\r226 ...\r227 ...\r228 ...\r229 ...\r230 ...\r231 ...\r232 ...\r233 ...\r234 ...\r235 ...\r236 ...\r237 ...\r238 ...\r239 ...\r240 ...\r241 ...\r242 ...\r243 ...\r244 ...\r245 ...\r246 ...\r247 ...\r248 ...\r249 ...\r250 ...\r251 ...\r252 ...\r253 ...\r254 ...\r255 ...\r256 ...\r257 ...\r258 ...\r259 ...\r260 ...\r261 ...\r262 ...\r263 ...\r264 ...\r265 ...\r266 ...\r267 ...\r268 ...\r269 ...\r270 ...\r271 ...\r272 ...\r273 ...\r274 ...\r275 ...\r276 ...\r277 ...\r278 ...\r279 ...\r280 ...\r281 ...\r282 ...\r283 ...\r284 ...\r285 ...\r286 ...\r287 ...\r288 ...\r289 ...\r290 ...\r291 ...\r292 ...\r293 ...\r294 ...\r295 ...\r296 ...\r297 ...\r298 ...\r299 ...\r300 ...\r301 ...\r302 ...\r303 ...\r304 ...\r305 ...\r306 ...\r307 ...\r308 ...\r309 ...\r310 ...\r311 ...\r312 ...\r313 ...\r314 ...\r315 ...\r316 ...\r317 ...\r318 ...\r319 ...\r320 ...\r321 ...\r322 ...\r323 ...\r324 ...\r325 ...\r326 ...\r327 ...\r328 ...\r329 ...\r330 ...\r331 ...\r332 ...\r333 ...\r334 ...\r335 ...\r336 ...\r337 ...\r338 ...\r339 ...\r340 ...\r341 ...\r342 ...\r343 ...\r344 ...\r345 ...\r346 ...\r347 ...\r348 ...\r349 ...\r350 ...\r351 ...\r352 ...\r353 ...\r354 ...\r355 ...\r356 ...\r357 ...\r358 ...\r359 ...\r360 ...\r361 ...\r362 ...\r363 ...\r364 ...\r365 ...\r366 ...\r367 ...\r368 ...\r369 ...\r370 ...\r371 ...\r372 ...\r373 ...\r374 ...\r375 ...\r376 ...\r377 ...\r378 ...\r379 ...\r380 ...\r381 ...\r382 ...\r383 ...\r384 ...\r385 ...\r386 ...\r387 ...\r388 ...\r389 ...\r390 ...\r391 ...\r392 ...\r393 ...\r394 ...\r395 ...\r396 ...\r397 ...\r398 ...\r399 ...\r400 ...\r401 ...\r402 ...\r403 ...\r404 ...\r405 ...\r406 ...\r407 ...\r408 ...\r409 ...\r410 ...\r411 ...\r412 ...\r413 ...\r414 ...\r415 ...\r416 ...\r417 ...\r418 ...\r419 ...\r420 ...\r421 ...\r422 ...\r423 ...\r424 ...\r425 ...\r426 ...\r427 ...\r428 ...\r429 ...\r430 ...\r431 ...\r432 ...\r433 ...\r434 ...\r435 ...\r436 ...\r437 ...\r438 ...\r439 ...\r440 ...\r441 ...\r442 ...\r443 ...\r444 ...\r445 ...\r446 ...\r447 ...\r448 ...\r449 ...\r450 ...\r451 ...\r452 ...\r453 ...\r454 ...\r455 ...\r456 ...\r457 ...\r458 ...\r459 ...\r460 ...\r461 ...\r462 ...\r463 ...\r464 ...\r465 ...\r466 ...\r467 ...\r468 ...\r469 ...\r470 ...\r471 ...\r472 ...\r473 ...\r474 ...\r475 ...\r476 ...\r477 ...\r478 ...\r479 ...\r480 ...\r481 ...\r482 ...\r483 ...\r484 ...\r485 ...\r486 ...\r487 ...\r488 ...\r489 ...\r490 ...\r491 ...\r492 ...\r493 ...\r494 ...\r495 ...\r496 ...\r497 ...\r498 ...\r499 ...\r500 ...\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/elasticsearch/connection/base.py:177: ElasticsearchDeprecationWarning: [types removal] Specifying types in bulk requests is deprecated.\n",
            "  warnings.warn(message, category=ElasticsearchDeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(25642, [])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PC_KspfwOiBB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "59560cd0-0e83-486b-977f-46fb6702adb0"
      },
      "source": [
        "from transformers.modeling_bert import BertForSequenceClassification\n",
        "from transformers.tokenization_bert import BertTokenizer\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 表示する選択肢の数\n",
        "OPTION_NUM = 10\n",
        "\n",
        "# Elasticsearchで検索する数(多くすると計算に時間がかかるようになります)\n",
        "SEARCH_NUM = 50\n",
        "\n",
        "class BertEvaluator:\n",
        "    def __init__(self):\n",
        "        # Googleの公開している事前学習済みのトークナイザとモデルをロード\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking', do_lower_case=False)\n",
        "        self.model = BertForSequenceClassification.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking', num_labels=2)\n",
        "        # self.model.to(\"cuda\")\n",
        "        # Google Colabでファインチューニングしたモデルをロード\n",
        "        self.model.load_state_dict(torch.load(\"bert_output/pytorch_model.bin\", map_location=\"cpu\"))\n",
        "\n",
        "    def evaluate(self, user_input, candidate):\n",
        "        with torch.no_grad():\n",
        "            # 発話のペアを特徴ベクトルに変換\n",
        "            tokenized = self.tokenizer([[user_input, candidate]], return_tensors=\"pt\")\n",
        "            input_ids = tokenized[\"input_ids\"]\n",
        "            token_type_ids = tokenized[\"token_type_ids\"]\n",
        "\n",
        "            # ファインチューニング済みのBERTを用いて特徴ベクトルから2文のスコアを計算\n",
        "            result = self.model.forward(input_ids, token_type_ids=token_type_ids)\n",
        "            # softmax関数によりスコアを正規化\n",
        "            result = F.softmax(result[0], dim=1).numpy().tolist()\n",
        "\n",
        "            # 結果を返す．\n",
        "            return result[0][1]\n",
        "\n",
        "# プログラムがエラーで落ちた場合，一時的にElasticsearchに接続できなくなりますが，一定時間経つことで接続可能になります．\n",
        "es = Elasticsearch()\n",
        "be = BertEvaluator()\n",
        "def get_reply(utterance, size=SEARCH_NUM):\n",
        "    results = es.search(index='dialogue_pair', body={'query':{'match':{'query':utterance}}, 'size':size,})\n",
        "\n",
        "    tmp_dict = {}\n",
        "    for r in results['hits']['hits']:\n",
        "        score = be.evaluate(utterance, r['_source']['response'])\n",
        "        tmp_dict[r['_source']['response']] = score\n",
        "    score_sorted = sorted(tmp_dict.items(), key=lambda x:x[1]*-1.0)\n",
        "    return [x[0] for x in score_sorted]\n",
        "\n",
        "res = None\n",
        "logs = []\n",
        "while(True):\n",
        "    u = input(\"\\n>\")\n",
        "    if \"exit\" == u:\n",
        "        break\n",
        "    elif u.isdecimal() and res is not None and int(u) < len(res):\n",
        "        u = res[int(u)]\n",
        "    elif \"back\" == u:\n",
        "        if len(logs) > 1:\n",
        "            logs.pop()\n",
        "            u = logs.pop()\n",
        "        else:\n",
        "            logs.pop()\n",
        "            continue\n",
        "\n",
        "\n",
        "    res = get_reply(u)\n",
        "    logs.append(u)\n",
        "    for i, l in enumerate(logs):\n",
        "        print(\"log \" + str(i) + \": \" , l)\n",
        "    for i, r in enumerate(res):\n",
        "        print(i, r)\n",
        "        if i >= OPTION_NUM:\n",
        "          break\n",
        "\n",
        "# 使用方法\n",
        "# 1. \">\"の右の入力欄にセリフを入力します\n",
        "# 2. 現在までのログと検索された候補が表示されます．\n",
        "# 3. 表示された候補の左の数字を\">\"の右の入力欄に入力することでその候補が次のセリフになります．数字以外のものを入力するとそれがセリフになります．\n",
        "# 4. 入力を間違えた場合，「back」と入力すると前回の状態に戻ることができます．\n",
        "# 5. 「exit」と入力すると終了します．"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            ">誰が犯人なんでしょうか？\n",
            "log 0:  誰が犯人なんでしょうか？\n",
            "0 別途からというと、君の覘っている犯人というのは誰だい\n",
            "1 いや君、あの男はまだ犯人とは決っていないよ\n",
            "2 犯人は誰だか知らない。だが犯人の居処だけは知っているのですよ……ホラ、ここに真暗な崩れ懸ったような倉庫がありますネ。犯人はこの中に居るのですよ\n",
            "3 犯人はまだ決定しとらん\n",
            "4 そうです。本当の勝見伍策は、たしかに殺人犯人ではありません。そしてたしかに彼は島に暮しています\n",
            "5 日本人が二人？　はてな、誰でしょうか。では、すぐ点呼をしてみましょう\n",
            "6 ポントス――つまりキャバレーの失踪した主人ですネ。部下は懸命に捜索に当っています。今明日中にきっと発見してみせますから\n",
            "7 わしは赤見沢が真実不能者かどうか、厳重に監視をしている。序に、あの女も小使夫婦も見張っている。赤見沢たちの犯行は、例の臼井という若僧や前知事の目賀野が出て来れば分ると思うんだが、どういうわけか彼等は姿を見せん。それはなぜだろうか、どうも分らない\n",
            "8 ああ、そうですか、\n",
            "9 では、家に居るのは本当の勝見ではなかったのですか、まア……。しかし一体あれは誰でございましたかしら\n",
            "10 黒河内さん。儂は警視総監じゃありませんよ。盗人の処分なんか、貴公の役目じゃありませんか\n",
            "\n",
            ">3\n",
            "log 0:  誰が犯人なんでしょうか？\n",
            "log 1:  犯人はまだ決定しとらん\n",
            "0 犯人はまだ決定しとらん\n",
            "1 別途からというと、君の覘っている犯人というのは誰だい\n",
            "2 いや君、あの男はまだ犯人とは決っていないよ\n",
            "3 えッ、犯人が判りましたか？　あの、井神陽吉が、では、犯人なのですか？\n",
            "4 犯人は誰だか知らない。だが犯人の居処だけは知っているのですよ……ホラ、ここに真暗な崩れ懸ったような倉庫がありますネ。犯人はこの中に居るのですよ\n",
            "5 そうです。本当の勝見伍策は、たしかに殺人犯人ではありません。そしてたしかに彼は島に暮しています\n",
            "6 ウン、決ったとまでは行かないんだが、重大なる容疑者を捕えて、今盛んに大江山君が訊問している\n",
            "7 オイ貴様、盗人みたいなやつだナ。そんな暇があるなら職務執行妨害罪というのを研究しておけよ\n",
            "8 赤外線男――でしょうナ\n",
            "9 まだつきません\n",
            "10 ああ、速水さんが真ちゃんを殺したの\n",
            "\n",
            ">6\n",
            "log 0:  誰が犯人なんでしょうか？\n",
            "log 1:  犯人はまだ決定しとらん\n",
            "log 2:  ウン、決ったとまでは行かないんだが、重大なる容疑者を捕えて、今盛んに大江山君が訊問している\n",
            "0 僕は反対だ。それは非常に重大なことと思うがね。窓の内側か外側か、どっちから撃ったかということで、容疑者の顔触れががらりと変るんではないかね\n",
            "1 ピストルは、やっぱりこの事件に重大な役割をつとめていると思う。だからそれに関する取調べは無駄ではないと思うよ\n",
            "2 何、博士を探しに行くというのですか\n",
            "3 あの事件のときの婦人探偵の一致した『否』という答申を侯爵家に報告したのは責任者の私だったんです。それでお分りでしょう。しかしあのときの田鶴子さんに対する見解が、今日も尚続いているとはいえません。私達は、ここで改めて田鶴子さんを観察する必要があります\n",
            "4 その通り。土居はあの夜、主人鶴彌に面接した最後の者でありますぞ。そして自分のハンドバグを残留してこの屋敷を飛出したほどの狼狽ぶりを示している。一体あの女のこの周章狼狽は何から起ったことでしょうか。これこそ乃ちあの女が当夜鶴彌に毒を盛ったことを示唆している。自分で毒を盛ったが、それに愕いて、急いで逃げ出した。そしてハンドバグを忘れて来てしまった\n",
            "5 なるほど。そうすると、土居三津子がどういう手段で旗田を殺害したかという証拠も欲しいわけだが、それは見つかったかね\n",
            "6 さあ、私には判りませんですが\n",
            "7 よし、その言葉を忘れるな\n",
            "8 それで、その仕事と言うのは……\n",
            "9 たとえば、ええと……あの婦人がなぜその男を恐れているのか、その根拠をはっきりついていませんね\n",
            "10 おい、もうよせというのに……。なあに、彼等は飛行島めごく一部分だけを知っているのにすぎない。だから秘密が洩れるといっても、飛行島全体の秘密がむきだしにわかるというのではない。それに、彼等には、相当の金をつかませて、かたく口止をするつもりだ。だから心配は少しもない\n",
            "\n",
            ">自白が取れると？\n",
            "log 0:  誰が犯人なんでしょうか？\n",
            "log 1:  犯人はまだ決定しとらん\n",
            "log 2:  ウン、決ったとまでは行かないんだが、重大なる容疑者を捕えて、今盛んに大江山君が訊問している\n",
            "log 3:  自白が取れると？\n",
            "0 いまさら取調べなんて迂濶千万ではありませんか。が、まあ取調べもいいでしょう。しかし市会の意思を蹂躙して上程をさせまいとするのはいかん。上程してみた上で、取調べの必要ができたからと云って、そこで延期を図ればよろしい\n",
            "1 これでは、取引ではありません。\n",
            "2 ……なるほど、こいつは面白い\n",
            "3 なに、撮影機のレンズのふたを取るのを忘れたというのか。それじゃ、あの息づまるような恐竜出現の大光景が、たった一こまもとれていないのかい。じょうだんじゃないぜ。生命がけで、こんな熱帯の孤島まで来て苦労しているのに……\n",
            "4 はてな。むらさきに白い四角形の旗というと\n",
            "5 へい。これですか、機械おおいは……\n",
            "6 ふうん。一つの有力なる手懸りだ\n",
            "7 というと……\n",
            "8 なんといわれます？\n",
            "9 ……拇指のない右腕が、あの火事場に転がっていた。そしていつの間にか見えなくなった。しかも焼け死んだ人間の心当りはないというのだから、面白い話じゃないか\n",
            "10 なッなんだって？\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
